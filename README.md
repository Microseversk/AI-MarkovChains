# Markov Joke Generator

`markov_jokes.py` — это самостоятельный скрипт на Python, который читает файл с анекдотами и строит простую модель на основе цепей Маркова. Затем модель выдаёт новые тексты, похожие на примеры в датасете. Никаких внешних библиотек кроме стандартных не требуется.

> **Коротко об идее:** цепь Маркова запоминает, какие слова чаще всего следуют друг за другом. Во время генерации она подбирает следующее слово, глядя на несколько предыдущих. Чем больше контекст (параметр `--order`), тем более связными получаются фразы, но тем больше данных нужно для обучения.

## Требования и формат данных
1. Python 3.8+.
2. Файл с анекдотами (по умолчанию `full_jokes.txt`). Каждый анекдот отделяется пустой строкой:
   ```
   Анекдот №1
   ещё строка первого анекдота

   Анекдот №2

   ...
   ```
3. Если текст в другой кодировке (например, CP1251), на чтение это влияет параметр `--encoding` и опция `--fallback-encoding`.

## Как работает скрипт
1. **Чтение**: файл читается построчно с учётом кодировок.
2. **Нормализация**: строки объединяются в анекдоты; при необходимости всё переводится в нижний регистр (`--lowercase`).
3. **Токенизация**: текст режется на слова и знаки препинания.
4. **Обучение**: строится словарь переходов вида «контекст → возможные продолжения».
5. **Генерация**: начинаем с маркера `<START>`, выбираем слова пока не встретим `<END>` или не достигнем лимитов по длине/числу предложений.

## Быстрый старт
```bash
# Базовый запуск: 3 анекдота по дефолтным настройкам
python markov_jokes.py --count 3

# Максимальная длина 40 токенов и два предложения в каждом тексте
python markov_jokes.py --max-length 40 --sentences-per-joke 2

# Используем готовый корпус в cp1251
python markov_jokes.py --encoding cp1251 --fallback-encoding utf-8

# Разбиваем на отдельные предложения и повторяем результат
python markov_jokes.py --split-sentences --seed 2024 --count 4

# Генерация только одного, но длинного анекдота из 4 предложений
python markov_jokes.py --count 1 --sentences-per-joke 4 --max-length 120
```

## Подробно об аргументах
| Аргумент | Что делает | Когда нужен |
| --- | --- | --- |
| `--dataset PATH` | Путь к файлу с исходными анекдотами. | Указать другой файл или расположение. |
| `--encoding ENCODING` | Основная кодировка чтения (по умолчанию `utf-8`). | Если файл точно в UTF‑8 или другой известной кодировке. |
| `--fallback-encoding ENCODING` | Запасная кодировка, если основная не подошла (по умолчанию `cp1251`). | Когда датасет в неизвестной кодировке, часто помогает CP1251. |
| `--errors MODE` | Стратегия обработки ошибок декодирования (`ignore`, `strict`, `replace`). | Для чувствительных корпусов лучше `strict`, иначе `ignore`. |
| `--order N` | Размер контекста (n-грамма). Дефолт 3. | `2` даёт больше случайности, `4+` — более точные фразы. |
| `--min-tokens N` | Минимальное количество токенов, чтобы использовать последовательность. | Отбрасывать слишком короткие записи или заголовки. |
| `--split-sentences` | Заставляет скрипт учиться на отдельных предложениях. | Когда весь анекдот слишком длинный и модель «теряется». |
| `--count N` | Сколько анекдотов сгенерировать. | Для получения подборки из N примеров. |
| `--max-length N` | Ограничение по числу токенов на анекдот. | Если цепь может «зациклиться» и выдавать бесконечные тексты. |
| `--sentences-per-joke N` | Сколько предложений стараться сделать в каждом анекдоте. | Полезно для коротких, но цельных историй. |
| `--no-context-carry` | Перезапускает контекст перед каждым предложением. | Когда предложения должны быть независимыми. |
| `--seed SEED` | Фиксирует генератор случайных чисел. | Чтобы воспроизвести удачную генерацию. |
| `--lowercase` | Переводит текст корпуса в нижний регистр. | Снижает количество редких вариантов написания. |

## Пример вывода
```
01: студенты спорят в очереди за кофе, кто первым выучит все билеты.
02: приходит муж домой, смотрит на кота и вспоминает, что обещал не играть в карты.
```
Число в начале строки — просто порядковый номер, чтобы легче было отбирать понравившиеся шутки.

## Частые сценарии
1. **Нужно больше связности.** Используйте `--split-sentences`, `--order 4` и увеличьте `--sentences-per-joke`.
2. **Слишком буквальная генерация.** Понизьте `--order` до 2 и отключите `--carry_context`.
3. **Подборка коротких шуток.**
   ```bash
   python markov_jokes.py --count 5 --split-sentences --sentences-per-joke 1 --max-length 25
   ```
4. **Эксперимент с разными корпусами.**
   ```bash
   python markov_jokes.py --dataset data/tech_jokes.txt --order 2 --count 10 --seed 77
   ```

## Полезные советы
- Если генерация выглядит «обрезанной», увеличьте `--max-length` или `--sentences-per-joke`.
- Когда модель повторяет одинаковые куски, значит, контекст слишком большой для объёма данных — уменьшайте `--order`.
- Всегда держите оригинал корпуса нетронутым и работайте с копией, если хотите экспериментировать с фильтрацией.
- Для оценки качества можно сохранять вывод в файл:
  ```bash
  python markov_jokes.py --count 20 > samples.txt
  ```

Теперь даже человек, который не знаком с цепями Маркова, сможет понять, что делает скрипт, и подобрать параметры под свои задачи. Удачных шуток! 
